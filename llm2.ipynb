{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f368b20-1cad-4922-9b96-f769d201837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: constitution_finetune.jsonl\n",
      "✅ Saved: ipc_finetune.jsonl\n",
      "✅ Saved: crpc_finetune.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Define input file paths\n",
    "input_files = {\n",
    "   \"constitution\": r\"C:\\Users\\DELL\\Downloads\\constitution_qa.json\",\n",
    "    \"ipc\": r\"C:\\Users\\DELL\\Downloads\\ipc_qa.json\",\n",
    "    \"crpc\": r\"C:\\Users\\DELL\\Downloads\\crpc_qa.json\"\n",
    "}\n",
    "\n",
    "# Convert each file to JSONL format for GPT fine-tuning\n",
    "for name, file in input_files.items():\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        qa_data = json.load(f)\n",
    "    \n",
    "    output_path = f\"{name}_finetune.jsonl\"\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for pair in qa_data:\n",
    "            prompt = f\"Q: {pair['question']}\\nA:\"\n",
    "            completion = f\" {pair['answer']}\"\n",
    "            json_line = json.dumps({\"prompt\": prompt, \"completion\": completion})\n",
    "            out.write(json_line + \"\\n\")\n",
    "    \n",
    "    print(f\"✅ Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b58b1a-3534-459a-8eff-b10268915512",
   "metadata": {},
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff71d54-2e63-446f-8c87-f814799682d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      " Q: What is the Indian Constitution?\n",
      "A: That is a question we have been waiting for.\n",
      "(LAUGHTER)\n",
      "\n",
      "We have been waiting for years for a Supreme Court Justice to make an opinion on our constitutional rights and protect them from the tyranny of the American system. And now that Justice Scalia is saying he is not going to follow this path, that's a big surprise.\n",
      "\n",
      "We are waiting for him to tell us what he thinks.\n",
      "\n",
      "(APPLAUSE)\n",
      "\n",
      "We are waiting for a Supreme Court Justice to make an opinion on our constitutional rights and protect them from the tyranny of the American system.\n",
      "\n",
      "(APPLAUSE)\n",
      "\n",
      "We have a constitutional court that is completely independent, and therefore it is free to act and to draw conclusions as to what a right a person has or is not entitled to, and that is a very important part of our country.\n",
      "\n",
      "(APPLAUSE)\n",
      "\n",
      "And we have a constitutional court, which is a very divided court of law, and we are asking for a Justice to write a opinion on what a right a person has or is not entitled to.\n",
      "\n",
      "(APPLAUSE)\n",
      "\n",
      "They are bound by the Constitution.\n",
      "\n",
      "(APPLAUSE)\n",
      "\n",
      "And I do think that the Constitution\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers torch\n",
    "\n",
    "# Step 2: Import required modules\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "import torch\n",
    "\n",
    "# Step 3: Load tokenizer and model safely\n",
    "try:\n",
    "    # Load GPT2 tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Necessary for padding\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Error loading model/tokenizer:\", e)\n",
    "    print(\"➡️ Check your internet connection or proxy settings.\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Create a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Step 5: Generate output from a prompt\n",
    "prompt = \"Q: What is the Indian Constitution?\\nA:\"\n",
    "output = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Step 6: Print result\n",
    "print(\"\\nGenerated Text:\\n\", output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b344238-d055-4b22-93f8-be67cb880578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d89ca-8109-4ce2-83f5-3f7849d669c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
