{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6c93fa-8136-4907-9b64-f6187fa80593",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'customer_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36636\\2293002998.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Combine the two datasets appropriately using 'customer_id' as the common column.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# An inner merge ensures that only customers present in both datasets are included,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# creating a comprehensive 360-degree view of the data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclaims_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcust_demographics_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'customer_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- Data Loading and Merging Complete ---\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mCombined DataFrame shape: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\\n\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'customer_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import base64\n",
    "\n",
    "# --- ML Model Imports ---\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Statistical Test Imports ---\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# --- Section 1: Data Loading and Merging ---\n",
    "# This section loads the two provided CSV files and merges them into a single DataFrame.\n",
    "\n",
    "# Fetch the content of the CSV files from the environment\n",
    "# Note: In a local Jupyter notebook, you would use pd.read_csv('claims.csv')\n",
    "# and pd.read_csv('cust_demographics.csv') directly.\n",
    "# Here, we simulate fetching file content as if from an external source.\n",
    "\n",
    "\n",
    "# Load the datasets into pandas DataFrames\n",
    "claims_df = pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\assignment\\Case Study 3 - Insurance Claims Case Study\\claims.csv\")\n",
    "cust_demographics_df = pd.read_csv(r\"C:\\Users\\DELL\\OneDrive\\Desktop\\assignment\\Case Study 3 - Insurance Claims Case Study\\cust_demographics.csv\")\n",
    "\n",
    "# Combine the two datasets appropriately using 'customer_id' as the common column.\n",
    "# An inner merge ensures that only customers present in both datasets are included,\n",
    "# creating a comprehensive 360-degree view of the data.\n",
    "combined_df = pd.merge(claims_df, cust_demographics_df, on='customer_id', how='inner')\n",
    "\n",
    "print(\"--- Data Loading and Merging Complete ---\")\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\\n\")\n",
    "\n",
    "\n",
    "# --- Section 2: Data Audit ---\n",
    "# This section performs a data audit to understand the structure, data types,\n",
    "# and presence of missing values and unique values in the combined dataset.\n",
    "\n",
    "data_audit = pd.DataFrame({\n",
    "    'Column': combined_df.columns,\n",
    "    'Dtype_Before': combined_df.dtypes,\n",
    "    'Non_Null_Count': combined_df.notnull().sum(),\n",
    "    'Null_Count': combined_df.isnull().sum(),\n",
    "    'Unique_Values': combined_df.nunique()\n",
    "})\n",
    "\n",
    "print(\"--- Data Audit (Sample) ---\")\n",
    "print(data_audit.to_markdown(index=False))\n",
    "print(\"\\nObservations from Data Audit:\")\n",
    "print(\"- `claim_amount` and `total_policy_claims` are `object` type, likely due to non-numeric characters or 'NA' values.\")\n",
    "print(\"- `claim_date` and `DateOfBirth` are `object` type and need conversion to `datetime` for date calculations.\")\n",
    "print(\"- Missing values are present in `claim_amount` and `total_policy_claims`.\\n\")\n",
    "\n",
    "\n",
    "# --- Section 3: Data Cleaning and Transformation ---\n",
    "# This section addresses data quality issues and transforms columns as required\n",
    "# for subsequent analysis.\n",
    "\n",
    "# 3. Convert the column 'claim_amount' to numeric.\n",
    "# Remove the '$' sign and then convert to float. 'errors=coerce' will turn\n",
    "# any values that cannot be converted into NaN.\n",
    "combined_df['claim_amount'] = combined_df['claim_amount'].astype(str).str.replace('$', '', regex=False)\n",
    "combined_df['claim_amount'] = pd.to_numeric(combined_df['claim_amount'], errors='coerce')\n",
    "\n",
    "# 4. Of all the injury claims, some of them have gone unreported with the police.\n",
    "# Create an alert flag (1,0) for all such claims.\n",
    "# First, standardize 'claim_category' based on 'claim_type' if 'injury' is present.\n",
    "combined_df['claim_category'] = combined_df['claim_type'].apply(lambda x: 'Injury' if 'injury' in str(x).lower() else 'Material')\n",
    "\n",
    "# Initialize the flag column with 0 (not unreported)\n",
    "combined_df['unreported_injury_flag'] = 0\n",
    "# Set the flag to 1 where claim_category is 'Injury' AND police_report is 'No'\n",
    "combined_df.loc[(combined_df['claim_category'] == 'Injury') & (combined_df['police_report'] == 'No'), 'unreported_injury_flag'] = 1\n",
    "print(\"--- Unreported Injury Flag Created ---\")\n",
    "print(f\"Number of unreported injury claims: {combined_df['unreported_injury_flag'].sum()}\\n\")\n",
    "\n",
    "# 5. Retain the most recent observation and delete any duplicated records in the data\n",
    "# based on the customer ID column.\n",
    "# Convert 'claim_date' to datetime for proper sorting.\n",
    "# Using format='%m/%d/%Y' to handle the specific date format in the CSV.\n",
    "combined_df['claim_date'] = pd.to_datetime(combined_df['claim_date'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "# Sort by customer_id and then by claim_date in descending order to ensure\n",
    "# the most recent claim comes first for each customer.\n",
    "combined_df = combined_df.sort_values(by=['customer_id', 'claim_date'], ascending=[True, False])\n",
    "\n",
    "# Drop duplicates based on 'customer_id', keeping the first occurrence (which is the most recent).\n",
    "initial_rows = combined_df.shape[0]\n",
    "combined_df = combined_df.drop_duplicates(subset='customer_id', keep='first')\n",
    "print(\"--- Duplicate Customer Records Handled ---\")\n",
    "print(f\"Removed {initial_rows - combined_df.shape[0]} duplicate customer records, keeping the most recent claim.\\n\")\n",
    "\n",
    "# 6. Check for missing values and impute the missing values with an appropriate value.\n",
    "# (mean for continuous and mode for categorical)\n",
    "print(\"--- Imputing Missing Values ---\")\n",
    "for column in combined_df.columns:\n",
    "    if combined_df[column].isnull().sum() > 0:\n",
    "        if pd.api.types.is_numeric_dtype(combined_df[column]):\n",
    "            # Impute with mean for numeric columns\n",
    "            mean_val = combined_df[column].mean()\n",
    "            combined_df[column].fillna(mean_val, inplace=True)\n",
    "            print(f\"  - Imputed numeric column '{column}' with mean: {mean_val:.2f}\")\n",
    "        else:\n",
    "            # Impute with mode for categorical columns\n",
    "            mode_val = combined_df[column].mode()[0]\n",
    "            combined_df[column].fillna(mode_val, inplace=True)\n",
    "            print(f\"  - Imputed categorical column '{column}' with mode: '{mode_val}'\")\n",
    "print(\"Missing value imputation complete.\\n\")\n",
    "\n",
    "# 7. Calculate the age of customers in years. Based on the age, categorize the customers.\n",
    "# Convert 'DateOfBirth' to datetime.\n",
    "# Using format='%d-%b-%y' to handle the specific date format in the CSV.\n",
    "combined_df['DateOfBirth'] = pd.to_datetime(combined_df['DateOfBirth'], errors='coerce', format='%d-%b-%y')\n",
    "\n",
    "# Use a fixed reference date for age calculation, as specified in question 9.\n",
    "reference_date = pd.to_datetime('2018-10-01')\n",
    "\n",
    "# Calculate age in years. Using 365.25 for leap years.\n",
    "combined_df['age'] = (reference_date - combined_df['DateOfBirth']).dt.days / 365.25\n",
    "\n",
    "# Define a function to categorize age based on the given criteria.\n",
    "def categorize_age(age):\n",
    "    if age < 18:\n",
    "        return 'Children'\n",
    "    elif 18 <= age < 30:\n",
    "        return 'Youth'\n",
    "    elif 30 <= age < 60:\n",
    "        return 'Adult'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "# Apply the categorization function to create the 'age_category' column.\n",
    "combined_df['age_category'] = combined_df['age'].apply(categorize_age)\n",
    "print(\"--- Age Calculated and Categorized ---\")\n",
    "print(\"Age categories created: Children (<18), Youth (18-30), Adult (30-60), Senior (>60).\\n\")\n",
    "\n",
    "\n",
    "# --- Section 4: Data Analysis Questions ---\n",
    "# This section answers specific analytical questions using the cleaned and transformed data.\n",
    "\n",
    "# 8. What is the average amount claimed by the customers from various segments?\n",
    "avg_claim_by_segment = combined_df.groupby('age_category')['claim_amount'].mean().reset_index()\n",
    "# Define a specific order for age categories for consistent display\n",
    "age_order = ['Children', 'Youth', 'Adult', 'Senior']\n",
    "avg_claim_by_segment['age_category'] = pd.Categorical(avg_claim_by_segment['age_category'], categories=age_order, ordered=True)\n",
    "avg_claim_by_segment = avg_claim_by_segment.sort_values('age_category')\n",
    "\n",
    "print(\"--- Average Claim Amount by Age Segment ---\")\n",
    "print(avg_claim_by_segment.to_markdown(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# 9. What is the total claim amount based on incident cause for all the claims that have been\n",
    "# done at least 20 days prior to 1st of October, 2018.\n",
    "filter_date = pd.to_datetime('2018-10-01') - pd.Timedelta(days=20)\n",
    "filtered_claims = combined_df[combined_df['claim_date'] <= filter_date]\n",
    "total_claim_by_incident_cause = filtered_claims.groupby('incident_cause')['claim_amount'].sum().reset_index()\n",
    "\n",
    "print(\"--- Total Claim Amount by Incident Cause (Filtered by Date) ---\")\n",
    "print(total_claim_by_incident_cause.to_markdown(index=False))\n",
    "print(\"\\n\")\n",
    "\n",
    "# 10. How many adults from TX, DE and AK claimed insurance for driver related issues and causes?\n",
    "# Based on the data audit and common incident causes, 'Driver error' and 'Other driver error'\n",
    "# are identified as driver-related issues.\n",
    "driver_related_causes = ['Driver error', 'Other driver error']\n",
    "\n",
    "adults_filtered = combined_df[\n",
    "    (combined_df['age_category'] == 'Adult') &\n",
    "    (combined_df['State'].isin(['TX', 'DE', 'AK'])) &\n",
    "    (combined_df['incident_cause'].isin(driver_related_causes))\n",
    "]\n",
    "# Count unique customer IDs to get the number of distinct adults.\n",
    "num_adults_driver_issues = adults_filtered['customer_id'].nunique()\n",
    "\n",
    "print(f\"--- Number of Adults from TX, DE, AK with Driver-Related Issues: {num_adults_driver_issues} ---\\n\")\n",
    "\n",
    "\n",
    "# --- Section 5: Data Visualization ---\n",
    "# This section generates the requested charts using Matplotlib and Seaborn.\n",
    "# Each chart is saved as a base64 encoded PNG image for display.\n",
    "\n",
    "# Dictionary to store base64 encoded images\n",
    "images = {}\n",
    "\n",
    "# 11. Draw a pie chart between the aggregated value of claim amount based on gender and segment.\n",
    "# Represent the claim amount as a percentage on the pie chart.\n",
    "claim_by_gender_segment = combined_df.groupby(['gender', 'age_category'])['claim_amount'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Calculate percentage for each slice for direct display on labels\n",
    "total_claim_amount_pie = claim_by_gender_segment['claim_amount'].sum()\n",
    "percentages_pie = (claim_by_gender_segment['claim_amount'] / total_claim_amount_pie) * 100\n",
    "# Create labels including gender, age category, and percentage\n",
    "labels_pie = [f\"{g} - {s} ({p:.1f}%)\" for g, s, p in zip(claim_by_gender_segment['gender'], claim_by_gender_segment['age_category'], percentages_pie)]\n",
    "\n",
    "plt.pie(claim_by_gender_segment['claim_amount'], labels=labels_pie, autopct='%1.1f%%', startangle=90, pctdistance=0.85)\n",
    "plt.title('Total Claim Amount by Gender and Age Segment')\n",
    "plt.axis('equal') # Ensures the pie chart is circular.\n",
    "plt.tight_layout() # Adjusts plot to ensure everything fits without overlapping.\n",
    "pie_chart_img = BytesIO()\n",
    "plt.savefig(pie_chart_img, format='png')\n",
    "pie_chart_img.seek(0)\n",
    "images['pie_chart'] = base64.b64encode(pie_chart_img.read()).decode('utf-8')\n",
    "plt.close() # Close the plot to free up memory\n",
    "\n",
    "print(\"--- Pie Chart Generated (Base64 Encoded) ---\")\n",
    "\n",
    "\n",
    "# 12. Among males and females, which gender had claimed the most for any type of driver related issues?\n",
    "# This metric can be compared using a bar chart.\n",
    "driver_claims_by_gender = combined_df[combined_df['incident_cause'].isin(driver_related_causes)]\n",
    "total_driver_claims_gender = driver_claims_by_gender.groupby('gender')['claim_amount'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='gender', y='claim_amount', data=total_driver_claims_gender, palette='viridis')\n",
    "plt.title('Total Claim Amount for Driver-Related Issues by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Total Claim Amount')\n",
    "plt.tight_layout()\n",
    "bar_gender_driver_img = BytesIO()\n",
    "plt.savefig(bar_gender_driver_img, format='png')\n",
    "bar_gender_driver_img.seek(0)\n",
    "images['bar_gender_driver'] = base64.b64encode(bar_gender_driver_img.read()).decode('utf-8')\n",
    "plt.close()\n",
    "\n",
    "print(\"--- Bar Chart (Driver Claims by Gender) Generated (Base64 Encoded) ---\")\n",
    "\n",
    "\n",
    "# 13. Which age group had the maximum fraudulent policy claims? Visualize it on a bar chart.\n",
    "# Assuming 'fraudulent' is the column indicating fraudulent claims ('Yes'/'No').\n",
    "# Convert 'Yes' to 1 and 'No' to 0 for numerical analysis.\n",
    "combined_df['fraudulent_policy'] = combined_df['fraudulent'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Filter for fraudulent claims only\n",
    "fraudulent_claims_by_age = combined_df[combined_df['fraudulent_policy'] == 1]\n",
    "# Count the number of fraudulent claims per age category\n",
    "fraudulent_claims_count_by_age = fraudulent_claims_by_age.groupby('age_category').size().reset_index(name='fraud_claim_count')\n",
    "\n",
    "# Ensure age categories are in a sensible order for plotting\n",
    "fraudulent_claims_count_by_age['age_category'] = pd.Categorical(fraudulent_claims_count_by_age['age_category'], categories=age_order, ordered=True)\n",
    "fraudulent_claims_count_by_age = fraudulent_claims_count_by_age.sort_values('age_category')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='age_category', y='fraud_claim_count', data=fraudulent_claims_count_by_age, palette='plasma')\n",
    "plt.title('Number of Fraudulent Claims by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Number of Fraudulent Claims')\n",
    "plt.tight_layout()\n",
    "bar_fraud_age_img = BytesIO()\n",
    "plt.savefig(bar_fraud_age_img, format='png')\n",
    "bar_fraud_age_img.seek(0)\n",
    "images['bar_fraud_age'] = base64.b64encode(bar_fraud_age_img.read()).decode('utf-8')\n",
    "plt.close()\n",
    "\n",
    "print(\"--- Bar Chart (Fraudulent Claims by Age) Generated (Base64 Encoded) ---\")\n",
    "\n",
    "\n",
    "# 14. Visualize the monthly trend of the total amount that has been claimed by the customers.\n",
    "# Ensure that on the “month” axis, the month is in a chronological order not alphabetical order.\n",
    "# Extract month and year as a Period object for chronological sorting\n",
    "combined_df['claim_month'] = combined_df['claim_date'].dt.to_period('M')\n",
    "# Group by month and sum claim amounts\n",
    "monthly_claim_trend = combined_df.groupby('claim_month')['claim_amount'].sum().reset_index()\n",
    "# Convert Period to string for plotting, preserving chronological order\n",
    "monthly_claim_trend['claim_month_str'] = monthly_claim_trend['claim_month'].astype(str)\n",
    "\n",
    "# Sort chronologically by the Period object, then use the string for plotting\n",
    "monthly_claim_trend = monthly_claim_trend.sort_values('claim_month')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='claim_month_str', y='claim_amount', data=monthly_claim_trend, marker='o')\n",
    "plt.title('Monthly Trend of Total Claim Amount')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Claim Amount')\n",
    "plt.xticks(rotation=45) # Rotate x-axis labels for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7) # Add a grid for better readability\n",
    "plt.tight_layout()\n",
    "line_monthly_trend_img = BytesIO()\n",
    "plt.savefig(line_monthly_trend_img, format='png')\n",
    "line_monthly_trend_img.seek(0)\n",
    "images['line_monthly_trend'] = base64.b64encode(line_monthly_trend_img.read()).decode('utf-8')\n",
    "plt.close()\n",
    "\n",
    "print(\"--- Line Chart (Monthly Trend) Generated (Base64 Encoded) ---\")\n",
    "\n",
    "\n",
    "# 15. What is the average claim amount for gender and age categories and suitably represent the above\n",
    "# using a facetted bar chart, one facet that represents fraudulent claims and the other for non-fraudulent claims.\n",
    "avg_claim_gender_age_fraud = combined_df.groupby(['gender', 'age_category', 'fraudulent_policy'])['claim_amount'].mean().reset_index()\n",
    "\n",
    "# Map the binary 'fraudulent_policy' (0/1) to descriptive labels for the facets.\n",
    "avg_claim_gender_age_fraud['fraud_status'] = avg_claim_gender_age_fraud['fraudulent_policy'].map({1: 'Fraudulent Claims', 0: 'Non-Fraudulent Claims'})\n",
    "\n",
    "# Ensure age categories are ordered correctly for plotting in the facets.\n",
    "avg_claim_gender_age_fraud['age_category'] = pd.Categorical(avg_claim_gender_age_fraud['age_category'], categories=age_order, ordered=True)\n",
    "# Sort for consistent plotting order across facets.\n",
    "avg_claim_gender_age_fraud = avg_claim_gender_age_fraud.sort_values(['fraud_status', 'gender', 'age_category'])\n",
    "\n",
    "# Create the facetted bar chart using Seaborn's catplot.\n",
    "g = sns.catplot(x='age_category', y='claim_amount', hue='gender', col='fraud_status',\n",
    "                data=avg_claim_gender_age_fraud, kind='bar', height=6, aspect=1.2, palette='muted',\n",
    "                order=age_order) # Use the predefined order for x-axis categories.\n",
    "g.set_axis_labels(\"Age Category\", \"Average Claim Amount\") # Set axis labels.\n",
    "g.set_titles(col_template=\"Fraud Status: {col_name}\") # Set titles for each facet.\n",
    "g.fig.suptitle('Average Claim Amount by Gender, Age Category, and Fraud Status', y=1.02) # Main title for the entire figure.\n",
    "plt.tight_layout()\n",
    "facet_bar_img = BytesIO()\n",
    "plt.savefig(facet_bar_img, format='png')\n",
    "facet_bar_img.seek(0)\n",
    "images['facet_bar'] = base64.b64encode(facet_bar_img.read()).decode('utf-8')\n",
    "plt.close()\n",
    "\n",
    "print(\"--- Facetted Bar Chart Generated (Base64 Encoded) ---\")\n",
    "\n",
    "\n",
    "# --- Section 6: Output Results ---\n",
    "# This section prints the base64 encoded images, which can be embedded in HTML\n",
    "# or Markdown to display the plots.\n",
    "\n",
    "# import json\n",
    "# print(\"\\n--- Plot Images (Base64) ---\")\n",
    "# # Print the dictionary of base64 encoded images as a JSON string.\n",
    "# # This allows for easy parsing and embedding of the images in the output.\n",
    "# print(json.dumps(images))\n",
    "\n",
    "\n",
    "# --- Section 7: Machine Learning Model for Fraud Prediction ---\n",
    "# This section builds and evaluates machine learning models to predict 'fraudulent_policy'.\n",
    "\n",
    "print(\"\\n--- Section 7: Machine Learning Model for Fraud Prediction ---\")\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Drop columns that are IDs, dates, or directly related to the target in a way that would cause data leakage.\n",
    "# Also drop columns that are not useful for prediction or are redundant after feature engineering.\n",
    "X = combined_df.drop(columns=[\n",
    "    'customer_id', 'claim_id', 'claim_date', 'DateOfBirth', 'fraudulent',\n",
    "    'fraudulent_policy', 'Contact', 'claim_type' # 'claim_type' is used to derive 'claim_category'\n",
    "])\n",
    "y = combined_df['fraudulent_policy']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\\n\")\n",
    "\n",
    "# Identify categorical and numerical features for preprocessing\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "print(f\"Categorical features: {list(categorical_features)}\")\n",
    "print(f\"Numerical features: {list(numerical_features)}\\n\")\n",
    "\n",
    "# Create a preprocessing pipeline\n",
    "# Numerical features will be scaled.\n",
    "# Categorical features will be one-hot encoded.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\\n\")\n",
    "\n",
    "# --- Model 1: Logistic Regression ---\n",
    "print(\"--- Training Logistic Regression Model ---\")\n",
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', LogisticRegression(solver='liblinear', random_state=42))])\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "y_prob_lr = lr_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nLogistic Regression Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_prob_lr):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "# --- Model 2: Random Forest Classifier ---\n",
    "print(\"\\n--- Training Random Forest Classifier Model ---\")\n",
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', RandomForestClassifier(random_state=42))])\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_prob_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nRandom Forest Classifier Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_prob_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "\n",
    "# --- Hyperparameter Tuning (Random Forest Example) ---\n",
    "print(\"\\n--- Hyperparameter Tuning (Random Forest Example using GridSearchCV) ---\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "# Reduced number of parameters for quicker demonstration\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100],\n",
    "    'classifier__max_depth': [None, 10],\n",
    "    'classifier__min_samples_leaf': [1, 5]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for exhaustive search\n",
    "grid_search_rf = GridSearchCV(rf_pipeline, param_grid_rf, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters found by GridSearchCV: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best ROC AUC score on training data: {grid_search_rf.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "y_prob_best_rf = best_rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nBest Random Forest Model Evaluation (after tuning):\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_best_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_best_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_best_rf):.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_prob_best_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best_rf))\n",
    "\n",
    "print(\"\\n--- Machine Learning Section Complete ---\")\n",
    "\n",
    "\n",
    "# --- Section 8: Hypothesis Testing ---\n",
    "# This section performs various hypothesis tests as requested.\n",
    "\n",
    "print(\"\\n--- Section 8: Hypothesis Testing ---\")\n",
    "hypothesis_results = {}\n",
    "alpha = 0.05 # Significance level\n",
    "\n",
    "# 16. Is there any similarity in the amount claimed by males and females?\n",
    "# Parameters: claim_amount (numerical), gender (categorical, 2 groups)\n",
    "# Test: Independent Two-Sample t-test\n",
    "males_claims = combined_df[combined_df['gender'] == 'Male']['claim_amount']\n",
    "females_claims = combined_df[combined_df['gender'] == 'Female']['claim_amount']\n",
    "\n",
    "# Perform Levene's test for equality of variances\n",
    "levene_stat, levene_p = stats.levene(males_claims, females_claims)\n",
    "equal_var = True if levene_p > alpha else False # If p-value > alpha, assume equal variances\n",
    "\n",
    "t_stat_gender, p_val_gender = stats.ttest_ind(males_claims, females_claims, equal_var=equal_var)\n",
    "\n",
    "hypothesis_results['gender_claim_amount'] = {\n",
    "    'test': 'Independent Two-Sample t-test',\n",
    "    'H0': 'The average claim amount for males is equal to the average claim amount for females.',\n",
    "    'H1': 'The average claim amount for males is not equal to the average claim amount for females.',\n",
    "    'alpha': alpha,\n",
    "    't_statistic': t_stat_gender,\n",
    "    'p_value': p_val_gender,\n",
    "    'equal_variance_assumed': equal_var\n",
    "}\n",
    "print(f\"\\n16. Similarity in amount claimed by males and females:\")\n",
    "print(f\"   - T-statistic: {t_stat_gender:.4f}, P-value: {p_val_gender:.4f}\")\n",
    "\n",
    "# 17. Is there any relationship between age category and segment?\n",
    "# Parameters: age_category (categorical), Segment (categorical)\n",
    "# Test: Chi-squared test of independence\n",
    "contingency_table_age_segment = pd.crosstab(combined_df['age_category'], combined_df['Segment'])\n",
    "chi2_stat_age_segment, p_val_age_segment, dof_age_segment, expected_age_segment = stats.chi2_contingency(contingency_table_age_segment)\n",
    "\n",
    "hypothesis_results['age_category_segment_relationship'] = {\n",
    "    'test': 'Chi-squared test of independence',\n",
    "    'H0': 'There is no relationship (independence) between age category and segment.',\n",
    "    'H1': 'There is a relationship (dependence) between age category and segment.',\n",
    "    'alpha': alpha,\n",
    "    'chi2_statistic': chi2_stat_age_segment,\n",
    "    'p_value': p_val_age_segment,\n",
    "    'degrees_of_freedom': dof_age_segment\n",
    "}\n",
    "print(f\"\\n17. Relationship between age category and segment:\")\n",
    "print(f\"   - Chi2-statistic: {chi2_stat_age_segment:.4f}, P-value: {p_val_age_segment:.4f}\")\n",
    "\n",
    "# 18. The current year has shown a significant rise in claim amounts as compared to 2016-17 fiscal average which was $10,000.\n",
    "# Parameters: Average claim amount in 2018 (numerical), known population mean ($10,000)\n",
    "# Test: One-Sample t-test (one-tailed)\n",
    "# Assuming \"current year\" refers to claims in 2018.\n",
    "claims_2018 = combined_df[combined_df['claim_date'].dt.year == 2018]['claim_amount']\n",
    "pop_mean = 10000\n",
    "\n",
    "# Perform one-sample t-test. 'alternative='greater'' for a one-tailed test (significant rise).\n",
    "t_stat_2018, p_val_2018 = stats.ttest_1samp(claims_2018, pop_mean, alternative='greater')\n",
    "\n",
    "hypothesis_results['2018_claim_rise'] = {\n",
    "    'test': 'One-Sample t-test (one-tailed)',\n",
    "    'H0': f'The average claim amount in 2018 is equal to or less than ${pop_mean}.',\n",
    "    'H1': f'The average claim amount in 2018 is significantly greater than ${pop_mean}.',\n",
    "    'alpha': alpha,\n",
    "    't_statistic': t_stat_2018,\n",
    "    'p_value': p_val_2018,\n",
    "    'sample_mean_2018': claims_2018.mean()\n",
    "}\n",
    "print(f\"\\n18. Significant rise in claim amounts in 2018 vs. $10,000:\")\n",
    "print(f\"   - Sample Mean (2018): {claims_2018.mean():.2f}\")\n",
    "print(f\"   - T-statistic: {t_stat_2018:.4f}, P-value: {p_val_2018:.4f}\")\n",
    "\n",
    "# 19. Is there any difference between age groups and insurance claims?\n",
    "# Parameters: claim_amount (numerical), age_category (categorical, >2 groups)\n",
    "# Test: ANOVA (Analysis of Variance)\n",
    "# Prepare data for ANOVA: list of claim amounts for each age category\n",
    "age_groups_claims = [combined_df[combined_df['age_category'] == category]['claim_amount'] for category in age_order]\n",
    "\n",
    "f_stat_anova, p_val_anova = stats.f_oneway(*age_groups_claims)\n",
    "\n",
    "hypothesis_results['age_group_claim_difference'] = {\n",
    "    'test': 'ANOVA (Analysis of Variance)',\n",
    "    'H0': 'The average claim amounts are equal across all age groups.',\n",
    "    'H1': 'At least one age group has a different average claim amount.',\n",
    "    'alpha': alpha,\n",
    "    'f_statistic': f_stat_anova,\n",
    "    'p_value': p_val_anova\n",
    "}\n",
    "print(f\"\\n19. Difference between age groups and insurance claims:\")\n",
    "print(f\"   - F-statistic: {f_stat_anova:.4f}, P-value: {p_val_anova:.4f}\")\n",
    "\n",
    "# 20. Is there any relationship between total number of policy claims and the claimed amount?\n",
    "# Parameters: total_policy_claims (numerical), claim_amount (numerical)\n",
    "# Test: Pearson Correlation\n",
    "# Ensure 'total_policy_claims' is numeric. It was already imputed and converted in Section 3.\n",
    "correlation_coeff, p_val_corr = stats.pearsonr(combined_df['total_policy_claims'], combined_df['claim_amount'])\n",
    "\n",
    "hypothesis_results['policy_claims_claim_amount_relationship'] = {\n",
    "    'test': 'Pearson Correlation Test',\n",
    "    'H0': 'There is no linear relationship between total number of policy claims and the claimed amount.',\n",
    "    'H1': 'There is a linear relationship between total number of policy claims and the claimed amount.',\n",
    "    'alpha': alpha,\n",
    "    'correlation_coefficient': correlation_coeff,\n",
    "    'p_value': p_val_corr\n",
    "}\n",
    "print(f\"\\n20. Relationship between total number of policy claims and claimed amount:\")\n",
    "print(f\"   - Pearson Correlation Coefficient: {correlation_coeff:.4f}, P-value: {p_val_corr:.4f}\")\n",
    "\n",
    "# Store hypothesis results for the markdown write-up\n",
    "import json\n",
    "print(\"\\n--- Hypothesis Test Results (JSON) ---\")\n",
    "print(json.dumps(hypothesis_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb4f6d-98ee-45f9-b5e1-87782d990b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
