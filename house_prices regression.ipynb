{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b43a30d-326d-4ba0-bf8d-ee9922734a03",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting House Prices Prediction Analysis...\n",
      "Files 'train (1).csv' and 'test (1).csv' loaded successfully.\n",
      "Combined data shape before preprocessing: (2919, 79)\n",
      "\n",
      "Missing values after initial imputation (should be empty if successful):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Missing values after feature engineering (should be empty):\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Columns identified for One-Hot Encoding: ['MSSubClass', 'MSZoning', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'Electrical', 'GarageType', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n",
      "Shape after One-Hot Encoding: (2919, 264)\n",
      "\n",
      "Missing values in combined data after ALL preprocessing steps:\n",
      "No remaining missing values found. Data is clean for modeling.\n",
      "\n",
      "Final X_train shape: (1460, 264)\n",
      "Final X_test shape: (1459, 264)\n",
      "Final y_train shape: (1460,)\n",
      "\n",
      "Generating Correlation Heatmap...\n",
      "Correlation Heatmap saved as 'Correlation_Heatmap.png'.\n",
      "\n",
      "Training and evaluating Ridge model...\n",
      "Ridge RMSE (mean): 0.1359\n",
      "Ridge RMSE (std): 0.0368\n",
      "\n",
      "Training and evaluating Lasso model...\n",
      "Lasso RMSE (mean): 0.1322\n",
      "Lasso RMSE (std): 0.0426\n",
      "\n",
      "Training and evaluating ElasticNet model...\n",
      "ElasticNet RMSE (mean): 0.1322\n",
      "ElasticNet RMSE (std): 0.0421\n",
      "\n",
      "Training and evaluating GradientBoosting model...\n",
      "GradientBoosting RMSE (mean): 0.1208\n",
      "GradientBoosting RMSE (std): 0.0234\n",
      "\n",
      "Training the final GradientBoostingRegressor model on the entire training data...\n",
      "\n",
      "Analysis complete. Submission file 'submission.csv' created successfully.\n",
      "First 5 rows of the submission file:\n",
      "     Id      SalePrice\n",
      "0  1461  125530.672743\n",
      "1  1462  156048.069457\n",
      "2  1463  188028.210365\n",
      "3  1464  192374.942169\n",
      "4  1465  188580.268590\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Starting House Prices Prediction Analysis...\")\n",
    "\n",
    "# --- 1. Data Loading ---\n",
    "# Load the datasets from the provided CSV files.\n",
    "try:\n",
    "    train_df_original = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\train (1).csv\")\n",
    "    test_df_original = pd.read_csv(r\"C:\\Users\\DELL\\Downloads\\test (1).csv\")\n",
    "    print(\"Files 'train (1).csv' and 'test (1).csv' loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}. Please ensure the files are accessible in the environment.\")\n",
    "    # Exit if critical files are not found\n",
    "    exit()\n",
    "\n",
    "# Store original IDs for submission and apply log transformation to SalePrice.\n",
    "# Log transformation is crucial because the evaluation metric (RMSE) is based on log-transformed prices.\n",
    "train_ids = train_df_original['Id']\n",
    "test_ids = test_df_original['Id']\n",
    "sale_price_log = np.log1p(train_df_original['SalePrice']) # log1p handles zero values gracefully\n",
    "\n",
    "# Drop 'Id' and 'SalePrice' (and the temporary 'SalePrice_Log' if it existed) from training data,\n",
    "# and 'Id' from test data before combining for preprocessing.\n",
    "train_features = train_df_original.drop(['Id', 'SalePrice'], axis=1)\n",
    "test_features = test_df_original.drop('Id', axis=1)\n",
    "\n",
    "# Concatenate train and test features for consistent preprocessing.\n",
    "all_data = pd.concat([train_features, test_features], axis=0).reset_index(drop=True)\n",
    "print(f\"Combined data shape before preprocessing: {all_data.shape}\")\n",
    "\n",
    "# --- 2. Missing Value Imputation ---\n",
    "# Impute missing values based on domain knowledge from data_description.txt.\n",
    "\n",
    "# 2.1. Features where 'NA' means 'None' (categorical absence)\n",
    "for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n",
    "            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "            'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtCond',\n",
    "            'MasVnrType']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "\n",
    "# 2.2. Numerical Features where 'NA' implies 0 (numerical absence)\n",
    "for col in ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n",
    "            'GarageCars', 'GarageArea', 'BsmtFullBath', 'BsmtHalfBath']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "\n",
    "# 2.3. LotFrontage: Impute with median grouped by Neighborhood\n",
    "# This is a more sophisticated imputation as LotFrontage can vary significantly by neighborhood.\n",
    "if 'LotFrontage' in all_data.columns:\n",
    "    all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "    # Fill any remaining NaNs (e.g., if a neighborhood has all missing LotFrontage) with overall median.\n",
    "    if all_data['LotFrontage'].isnull().any():\n",
    "        all_data['LotFrontage'] = all_data['LotFrontage'].fillna(all_data['LotFrontage'].median())\n",
    "\n",
    "# 2.4. GarageYrBlt: Fill with 0 for no garage, otherwise with median of existing garage years.\n",
    "if 'GarageYrBlt' in all_data.columns:\n",
    "    # If GarageType is 'None', it means no garage, so set GarageYrBlt to 0.\n",
    "    # Otherwise, fill with the median of existing GarageYrBlt values.\n",
    "    all_data['GarageYrBlt'] = np.where(all_data['GarageType'] == 'None',\n",
    "                                       0,\n",
    "                                       all_data['GarageYrBlt'].fillna(all_data['GarageYrBlt'].median()))\n",
    "\n",
    "# 2.5. Other categorical features with few missing values (Mode imputation)\n",
    "for col in ['MSZoning', 'Utilities', 'Electrical', 'KitchenQual', 'Exterior1st',\n",
    "            'Exterior2nd', 'SaleType', 'Functional']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n",
    "\n",
    "print(\"\\nMissing values after initial imputation (should be empty if successful):\")\n",
    "print(all_data.isnull().sum()[all_data.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering ---\n",
    "# Create new features to capture more complex relationships in the data.\n",
    "\n",
    "# 3.1. Total Square Footage (Above Ground + Basement)\n",
    "all_data['TotalSF'] = all_data['GrLivArea'] + all_data['TotalBsmtSF']\n",
    "\n",
    "# 3.2. Total Bathrooms\n",
    "all_data['TotalBath'] = all_data['FullBath'] + (all_data['HalfBath'] * 0.5) + \\\n",
    "                        all_data['BsmtFullBath'] + (all_data['BsmtHalfBath'] * 0.5)\n",
    "\n",
    "# 3.3. Total Porch Area (sum of all porch types)\n",
    "all_data['TotalPorchSF'] = all_data['OpenPorchSF'] + all_data['EnclosedPorch'] + \\\n",
    "                           all_data['3SsnPorch'] + all_data['ScreenPorch']\n",
    "\n",
    "# 3.4. Age-related Features: Years since built and years since last remodel.\n",
    "all_data['YearsBuilt'] = all_data['YrSold'] - all_data['YearBuilt']\n",
    "all_data['YearsRemod'] = all_data['YrSold'] - all_data['YearRemodAdd']\n",
    "\n",
    "# Correct for potential negative years (e.g., if YrSold < YearBuilt/RemodAdd for new builds or data errors)\n",
    "# Set negative values to 0, assuming it means a very new or just remodeled house.\n",
    "all_data['YearsBuilt'] = all_data['YearsBuilt'].apply(lambda x: x if x >= 0 else 0)\n",
    "all_data['YearsRemod'] = all_data['YearsRemod'].apply(lambda x: x if x >= 0 else 0)\n",
    "\n",
    "# 3.5. Simple Binary Features (presence/absence of certain amenities)\n",
    "all_data['HasPool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['Has2ndFloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['HasGarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['HasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "all_data['HasFireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(\"\\nMissing values after feature engineering (should be empty):\")\n",
    "print(all_data.isnull().sum()[all_data.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "# --- 4. Categorical Encoding ---\n",
    "# Convert categorical features into numerical format for model training.\n",
    "\n",
    "# 4.1. Convert some numerical features that are actually categorical to 'object' type\n",
    "# This ensures they are treated as categories during one-hot encoding.\n",
    "all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n",
    "all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "\n",
    "# 4.2. Ordinal Encoding: Map categorical features with a clear order to numerical values.\n",
    "# Mappings are based on the data_description.txt and common sense for quality/condition.\n",
    "\n",
    "# General Quality/Condition: Ex > Gd > TA > Fa > Po > None\n",
    "quality_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "for col in ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu',\n",
    "            'GarageQual', 'GarageCond', 'PoolQC']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(quality_map)\n",
    "\n",
    "# Basement Quality/Condition/Exposure\n",
    "bsmt_qual_cond_map = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0}\n",
    "bsmt_exposure_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n",
    "bsmt_fin_type_map = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0}\n",
    "\n",
    "for col in ['BsmtQual', 'BsmtCond']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(bsmt_qual_cond_map)\n",
    "if 'BsmtExposure' in all_data.columns:\n",
    "    all_data['BsmtExposure'] = all_data['BsmtExposure'].map(bsmt_exposure_map)\n",
    "for col in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data[col].map(bsmt_fin_type_map)\n",
    "\n",
    "# Functional: Typ > Min1 > Min2 > Mod > Maj1 > Maj2 > Sev > Sal\n",
    "functional_map = {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}\n",
    "if 'Functional' in all_data.columns:\n",
    "    all_data['Functional'] = all_data['Functional'].map(functional_map)\n",
    "\n",
    "# GarageFinish: Fin > RFn > Unf > None\n",
    "garage_finish_map = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0}\n",
    "if 'GarageFinish' in all_data.columns:\n",
    "    all_data['GarageFinish'] = all_data['GarageFinish'].map(garage_finish_map)\n",
    "\n",
    "# PavedDrive: Y > P > N\n",
    "paved_drive_map = {'Y': 2, 'P': 1, 'N': 0}\n",
    "if 'PavedDrive' in all_data.columns:\n",
    "    all_data['PavedDrive'] = all_data['PavedDrive'].map(paved_drive_map)\n",
    "\n",
    "# Fence: GdPrv > MnPrv > GdWo > MnWw > None\n",
    "fence_map = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'None': 0}\n",
    "if 'Fence' in all_data.columns:\n",
    "    all_data['Fence'] = all_data['Fence'].map(fence_map)\n",
    "\n",
    "# LandSlope: Gtl > Mod > Sev\n",
    "land_slope_map = {'Gtl': 2, 'Mod': 1, 'Sev': 0}\n",
    "if 'LandSlope' in all_data.columns:\n",
    "    all_data['LandSlope'] = all_data['LandSlope'].map(land_slope_map)\n",
    "\n",
    "# LotShape: Reg > IR1 > IR2 > IR3\n",
    "lot_shape_map = {'Reg': 3, 'IR1': 2, 'IR2': 1, 'IR3': 0}\n",
    "if 'LotShape' in all_data.columns:\n",
    "    all_data['LotShape'] = all_data['LotShape'].map(lot_shape_map)\n",
    "\n",
    "# Utilities: AllPub > NoSeWa > NoSewr (assuming hierarchy)\n",
    "utilities_map = {'AllPub': 3, 'NoSeWa': 2, 'NoSewr': 1, 'None': 0}\n",
    "if 'Utilities' in all_data.columns:\n",
    "    all_data['Utilities'] = all_data['Utilities'].map(utilities_map)\n",
    "\n",
    "# Street: Pave > Grvl\n",
    "street_map = {'Pave': 1, 'Grvl': 0}\n",
    "if 'Street' in all_data.columns:\n",
    "    all_data['Street'] = all_data['Street'].map(street_map)\n",
    "\n",
    "# 4.3. One-Hot Encoding for remaining nominal categorical features\n",
    "# Select all object type columns that are still present after ordinal mapping.\n",
    "one_hot_cols = all_data.select_dtypes(include='object').columns.tolist()\n",
    "print(f\"\\nColumns identified for One-Hot Encoding: {one_hot_cols}\")\n",
    "\n",
    "all_data = pd.get_dummies(all_data, columns=one_hot_cols, dummy_na=False)\n",
    "print(f\"Shape after One-Hot Encoding: {all_data.shape}\")\n",
    "\n",
    "# Final check for any remaining missing values after ALL preprocessing steps.\n",
    "# This is a critical step to ensure models don't fail due to NaNs.\n",
    "print(\"\\nMissing values in combined data after ALL preprocessing steps:\")\n",
    "missing_final = all_data.isnull().sum()\n",
    "missing_final = missing_final[missing_final > 0].sort_values(ascending=False)\n",
    "if not missing_final.empty:\n",
    "    print(missing_final)\n",
    "    # If there are any remaining missing values, fill them with 0 as a last resort.\n",
    "    # This might happen if a new category appeared in test set not in train and was not handled.\n",
    "    print(\"\\nFilling any remaining missing values with 0.\")\n",
    "    all_data.fillna(0, inplace=True)\n",
    "else:\n",
    "    print(\"No remaining missing values found. Data is clean for modeling.\")\n",
    "\n",
    "\n",
    "# Separate the preprocessed data back into training and test sets.\n",
    "X_train = all_data.iloc[:len(train_df_original)]\n",
    "X_test = all_data.iloc[len(train_df_original):]\n",
    "y_train = sale_price_log # Our log-transformed SalePrice\n",
    "\n",
    "print(f\"\\nFinal X_train shape: {X_train.shape}\")\n",
    "print(f\"Final X_test shape: {X_test.shape}\")\n",
    "print(f\"Final y_train shape: {y_train.shape}\")\n",
    "\n",
    "# --- 5. Visualization: Correlation Heatmap ---\n",
    "print(\"\\nGenerating Correlation Heatmap...\")\n",
    "\n",
    "# Combine X_train and y_train for correlation calculation\n",
    "train_data_for_corr = X_train.copy()\n",
    "train_data_for_corr['SalePrice_Log'] = y_train\n",
    "\n",
    "# Calculate correlations with SalePrice_Log\n",
    "correlations = train_data_for_corr.corr()['SalePrice_Log'].sort_values(ascending=False)\n",
    "\n",
    "# Select top N positive and negative correlated features (e.g., top 10 positive and top 10 negative)\n",
    "# Exclude SalePrice_Log itself from the top list, but include it in the final heatmap\n",
    "top_n = 10\n",
    "top_correlated_features = correlations.head(top_n).index.tolist() + correlations.tail(top_n).index.tolist()\n",
    "# Ensure SalePrice_Log is in the list for the heatmap\n",
    "if 'SalePrice_Log' not in top_correlated_features:\n",
    "    top_correlated_features.append('SalePrice_Log')\n",
    "\n",
    "# Create a subset DataFrame with only the selected features for the heatmap\n",
    "corr_matrix_subset = train_data_for_corr[top_correlated_features].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix_subset, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title(f'Correlation Heatmap of Top {len(top_correlated_features)-1} Features with Log-transformed SalePrice', fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Correlation_Heatmap.png') # Save the plot\n",
    "plt.close() # Close the plot to free memory\n",
    "print(\"Correlation Heatmap saved as 'Correlation_Heatmap.png'.\")\n",
    "\n",
    "\n",
    "# --- 6. Model Training and Evaluation ---\n",
    "\n",
    "# Define a function for Root Mean Squared Error with Cross-Validation (RMSE_CV)\n",
    "# This helps in evaluating model performance robustly and avoiding overfitting.\n",
    "def rmse_cv(model, X, y):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42) # 10-fold cross-validation\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return(rmse)\n",
    "\n",
    "# Initialize models.\n",
    "# Note: XGBoost and LightGBM are commented out due to previous ModuleNotFoundError.\n",
    "# If available, they are highly recommended for this type of problem.\n",
    "ridge = Ridge(alpha=10)\n",
    "lasso = Lasso(alpha=0.0005, max_iter=10000)\n",
    "elasticnet = ElasticNet(alpha=0.0005, l1_ratio=0.9, max_iter=10000)\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                max_depth=4, max_features='sqrt',\n",
    "                                min_samples_leaf=15, min_samples_split=10,\n",
    "                                loss='huber', random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Ridge': ridge,\n",
    "    'Lasso': lasso,\n",
    "    'ElasticNet': elasticnet,\n",
    "    'GradientBoosting': gbr\n",
    "}\n",
    "\n",
    "rmse_scores = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining and evaluating {name} model...\")\n",
    "    score = rmse_cv(model, X_train, y_train)\n",
    "    rmse_scores[name] = score.mean()\n",
    "    print(f\"{name} RMSE (mean): {score.mean():.4f}\")\n",
    "    print(f\"{name} RMSE (std): {score.std():.4f}\")\n",
    "\n",
    "# --- 7. Final Model Training and Prediction ---\n",
    "\n",
    "# Based on typical performance in similar competitions, GradientBoostingRegressor is a strong choice.\n",
    "# For a more advanced solution, an ensemble (stacking/blending) of the best performing models would be ideal.\n",
    "print(\"\\nTraining the final GradientBoostingRegressor model on the entire training data...\")\n",
    "final_model = gbr\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the preprocessed test data.\n",
    "predictions_log = final_model.predict(X_test)\n",
    "\n",
    "# Convert predictions back from log scale to original price scale using np.expm1.\n",
    "predictions = np.expm1(predictions_log)\n",
    "\n",
    "# --- 8. Create Submission File ---\n",
    "# Format the predictions into the required submission file format.\n",
    "submission_df = pd.DataFrame({'Id': test_ids, 'SalePrice': predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nAnalysis complete. Submission file 'submission.csv' created successfully.\")\n",
    "print(\"First 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f660d-2f3e-4875-abc7-52e5975b73be",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
